<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
"http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en">
<head>
<!-- 2015-11-26 Thu 00:53 -->
<meta  http-equiv="Content-Type" content="text/html;charset=utf-8" />
<meta  name="viewport" content="width=device-width, initial-scale=1" />
<title>Note Of Deep Learning Course</title>
<meta  name="generator" content="Org-mode" />
<meta  name="author" content="Spinpx" />
<link rel="stylesheet" type="text/css" href="http://spinpx.com/org.css"/>
<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        displayAlign: "center",
        displayIndent: "0em",

        "HTML-CSS": { scale: 100,
                        linebreaks: { automatic: "false" },
                        webFont: "TeX"
                       },
        SVG: {scale: 100,
              linebreaks: { automatic: "false" },
              font: "TeX"},
        NativeMML: {scale: 100},
        TeX: { equationNumbers: {autoNumber: "AMS"},
               MultLineWidth: "85%",
               TagSide: "right",
               TagIndent: ".8em"
             }
});
</script>
<script type="text/javascript"
        src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS_HTML"></script>
</head>
<body>
<div id="preamble" class="status">
<div class = "nav">
    <ul>
    <li><a class = "logo" href="/index.html">SPinPx</a><li>
	<li><a href="/posts.html"> Posts </a></li>
	<li><a href="/notes.html"> Notes </a></li>
	<li><a href="/gists.html"> Gists </a></li>
	<li><a href="/about.html"> About </a></li>
    </ul>
</div>
</div>
<div id="content">
<h1 class="title">Note Of Deep Learning Course</h1>
<div id="table-of-contents">
<h2>Table of Contents</h2>
<div id="text-table-of-contents">
<ul>
<li><a href="#orgheadline8">Introduction <span class="timestamp-wrapper"><span class="timestamp">&lt;2015-09-24 Thu&gt;</span></span></a>
<ul>
<li><a href="#orgheadline1">resource</a></li>
<li><a href="#orgheadline5">optimization</a>
<ul>
<li><a href="#orgheadline2">gradient descent</a></li>
<li><a href="#orgheadline3">least square revisit</a></li>
<li><a href="#orgheadline4">MLE</a></li>
</ul>
</li>
<li><a href="#orgheadline7">classification &amp;  logistic regression</a>
<ul>
<li><a href="#orgheadline6">classification</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#orgheadline16">Neural Network <span class="timestamp-wrapper"><span class="timestamp">&lt;2015-10-08 Thu&gt;</span></span></a>
<ul>
<li><a href="#orgheadline15">Neuron</a>
<ul>
<li><a href="#orgheadline9">activation function, f</a></li>
<li><a href="#orgheadline10">hidden-layer neural network</a></li>
<li><a href="#orgheadline11">Back propagation NN (BP NN)</a></li>
<li><a href="#orgheadline13">Auto-Encoder</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#orgheadline17">CNN <span class="timestamp-wrapper"><span class="timestamp">&lt;2015-10-15 Thu&gt;</span></span></a></li>
<li><a href="#orgheadline22">Tricks<span class="timestamp-wrapper"><span class="timestamp">&lt;2015-10-29 Thu&gt;</span></span></a>
<ul>
<li><a href="#orgheadline18">dot not convergence</a></li>
<li><a href="#orgheadline19">learning rate</a></li>
<li><a href="#orgheadline20">GD method</a></li>
<li><a href="#orgheadline21">Initialization</a></li>
</ul>
</li>
</ul>
</div>
</div>

<div id="outline-container-orgheadline8" class="outline-2">
<h2 id="orgheadline8">Introduction <span class="timestamp-wrapper"><span class="timestamp">&lt;2015-09-24 Thu&gt;</span></span></h2>
<div class="outline-text-2" id="text-orgheadline8">
</div><div id="outline-container-orgheadline1" class="outline-3">
<h3 id="orgheadline1">resource</h3>
<div class="outline-text-3" id="text-orgheadline1">
<ul class="org-ul">
<li>deeplearning.net</li>
<li>NIPS, ICML, CVPR, ICCV, ECCV, AISTAD, AISSTATS</li>
</ul>
</div>
</div>
<div id="outline-container-orgheadline5" class="outline-3">
<h3 id="orgheadline5">optimization</h3>
<div class="outline-text-3" id="text-orgheadline5">
</div><div id="outline-container-orgheadline2" class="outline-4">
<h4 id="orgheadline2">gradient descent</h4>
<div class="outline-text-4" id="text-orgheadline2">
<ul class="org-ul">
<li>Note taken on <span class="timestamp-wrapper"><span class="timestamp">[2015-09-24 Thu 16:21] </span></span> <br  />
review of matrix</li>
</ul>
<ul class="org-ul">
<li>use all instance to optimize</li>
<li>only use one instance in each update
<ul class="org-ul">
<li>stochastic gradient descent</li>
<li>solve the problem and fastest</li>
</ul></li>
<li>do not ensure the solution has the best result</li>
</ul>
</div>
</div>
<div id="outline-container-orgheadline3" class="outline-4">
<h4 id="orgheadline3">least square revisit</h4>
</div>
<div id="outline-container-orgheadline4" class="outline-4">
<h4 id="orgheadline4">MLE</h4>
<div class="outline-text-4" id="text-orgheadline4">
<ul class="org-ul">
<li>Gauss distribution -&gt; linear regression</li>
</ul>
</div>
</div>
</div>



<div id="outline-container-orgheadline7" class="outline-3">
<h3 id="orgheadline7">classification &amp;  logistic regression</h3>
<div class="outline-text-3" id="text-orgheadline7">
</div><div id="outline-container-orgheadline6" class="outline-4">
<h4 id="orgheadline6">classification</h4>
<div class="outline-text-4" id="text-orgheadline6">
<ul class="org-ul">
<li>Note taken on <span class="timestamp-wrapper"><span class="timestamp">[2015-09-24 Thu 17:26] </span></span> <br  />
figure out the three regressions</li>
</ul>
<ul class="org-ul">
<li>n dimension -&gt; 1 dimension</li>
<li>sigmoid function, logistic function
<ul class="org-ul">
<li>discrete &lt;-&gt; sequential</li>
</ul></li>
<li>MLE</li>

<li>? (gradient descent) -&gt; logistic regression</li>

<li>generalization -&gt; softmax regression</li>
</ul>


<p>
T.E.P
based on P, learn a mapping from raw data to the output.
</p>

<p>
how to map? deep learning
</p>
</div>
</div>
</div>
</div>


<div id="outline-container-orgheadline16" class="outline-2">
<h2 id="orgheadline16">Neural Network <span class="timestamp-wrapper"><span class="timestamp">&lt;2015-10-08 Thu&gt;</span></span></h2>
<div class="outline-text-2" id="text-orgheadline16">
</div><div id="outline-container-orgheadline15" class="outline-3">
<h3 id="orgheadline15">Neuron</h3>
<div class="outline-text-3" id="text-orgheadline15">
</div><div id="outline-container-orgheadline9" class="outline-4">
<h4 id="orgheadline9">activation function, f</h4>
<div class="outline-text-4" id="text-orgheadline9">
<ul class="org-ul">
<li>sigmoid function</li>
<li>tanh</li>
<li>rectified linear units (Relu) :
<ul class="org-ul">
<li>fx = max(0,x)</li>
<li>sparsity</li>
<li>converge fast</li>
</ul></li>
<li>leaky Relu:
<ul class="org-ul">
<li>f = (x&lt;0) ax , f = (x &gt;0) x</li>
<li>a to be optimized (fix one ?)</li>
</ul></li>
<li>Max out
<ul class="org-ul">
<li>maxout network</li>
<li>fx = max(w1x+b, w2x+b)</li>
</ul></li>
</ul>


<ul class="org-ul">
<li>choice order?
<ol class="org-ol">
<li>Relu (leaky , maxout)</li>
<li>tanh</li>
<li>sigmod</li>
</ol></li>
</ul>


<ul class="org-ul">
<li>w0x0  = b (bias)</li>

<li>f<sub>x</sub> = f(&sum;<sub>i</sub> w<sub>i</sub> x<sub>i</sub> + b)</li>
</ul>
</div>
</div>

<div id="outline-container-orgheadline10" class="outline-4">
<h4 id="orgheadline10">hidden-layer neural network</h4>
<div class="outline-text-4" id="text-orgheadline10">
<ul class="org-ul">
<li><a href="https://class.coursera.org/ml-005/lecture">https://class.coursera.org/ml-005/lecture</a></li>
</ul>

<pre class="example">
x1, x2, x3  input
|    |   |   
_____________
|h1, h2 , h3|  hidden layer   # nerons:  ?parameter
-------------
   |     |
sunny(o1) rainy(o2)
</pre>


<ul class="org-ul">
<li>h<sub>1</sub>' = f(&sum; w<sub>1i</sub> x<sub>i</sub> + b<sub>1</sub>)</li>
<li>h<sub>2</sub>' = f(&sum; w<sub>2i</sub> x<sub>i</sub> + b<sub>2</sub>)</li>
<li>&#x2026;.</li>
<li>h<sub>4</sub>' = f(&sum; w<sub>4i</sub> x<sub>i</sub> + b<sub>4</sub>)</li>
</ul>


<ul class="org-ul">
<li>A1 = W1X0 + b</li>
<li>Z(h1,,,h4) = ?</li>
</ul>


<ul class="org-ul">
<li>o<sub>1</sub>' = f(&sum; w<sub>i1</sub><sup>2</sup> h<sub>i</sub> + b<sub>1</sub><sup>2</sup>)</li>
<li>o<sub>2</sub>' = f(&sum; w<sub>i2</sub><sup>2</sup> h<sub>i</sub> + b<sub>2</sub><sup>2</sup>)</li>

<li>single layer perception</li>
<li>multiple layer perception</li>
</ul>


<ul class="org-ul">
<li>feed forward neuron network</li>
<li>back propagation NN (x, output given , to solve w, b)
<ul class="org-ul">
<li>CNN, RNN</li>
</ul></li>
</ul>
</div>
</div>

<div id="outline-container-orgheadline11" class="outline-4">
<h4 id="orgheadline11">Back propagation NN (BP NN)</h4>
<div class="outline-text-4" id="text-orgheadline11">
<ul class="org-ul">
<li>A &sdot; B = &sum;<sub>ij</sub> A<sub>ij</sub> B<sub>ij</sub>  (Frobisher product)</li>

<li>(A o B)<sub>ij</sub> = A<sub>ij</sub> B<sub>ij</sub>  (hadamard product)</li>

<li>A &sdot; (B o C) = (A o B) &sdot; C</li>

<li>A &sdot; (BC) = (B<sup>T</sup> A) &sdot; C = (AC<sup>T</sup>) &sdot; B</li>
</ul>



<ul class="org-ul">
<li>Y = fx 
=&gt; dY = f'x o dx</li>
<li>Let y  = h(x1, x2, &#x2026; , xk)  #output function</li>
<li>dy = &sum; _{k] A<sub>k</sub> &sdot; dx<sub>k</sub></li>
</ul>
<p>
&lt;===&gt;   &nabla; y = (&alpha; y / &alpha; x<sub>1</sub>, &#x2026;. &alpha; y / &alpha; x<sub>k</sub>)
                = (A1 , &#x2026; Ak)
</p>
</div>


<ul class="org-ul"><li><a id="orgheadline12"></a>k layer NN<br  /><div class="outline-text-5" id="text-orgheadline12">
<ul class="org-ul">
<li>k hidden layer: n0 ,,,,, nk</li>
<li>regression peoblem</li>
</ul>
<pre class="example">
n0, n1 ,n2 ,,, nk
  ||  ||      ||
  w1  w2      wk
</pre>

<ul class="org-ul">
<li>Zj = 
x, j=0
f_({j}(A<sub>j</sub>), j&gt;0, where A<sub>j</sub> = W<sub>j</sub> Z<sub>j-1</sub> + b<sub>j</sub> 1<sup>T</sup></li>

<li>Wj &isin; R<sup>n<sup>j</sup> x n<sup>j-1</sup></sup></li>
<li>bj &isin; R<sup>n<sup>j</sup></sup></li>

<li>regression:  x -&gt; g(x) min<sub>&theta;</sub> || Zk - Y || ^{2}<sub>F</sub></li>
<li>given f,x,y,</li>
<li>unknown: &theta; = [w1, &#x2026;., wk, b1&#x2026;.., bk]</li>

<li>J(w, b) =  (1/t) 1<sup>T</sup> l(Zk)</li>

<li>&delta; J = &delta; k &sdot; d Ak,</li>
<li>Ak = WkZ<sub>k-1</sub> + b<sub>k</sub> 1<sup>T</sup></li>
<li>&delta; k = (1\t) &nabla; l(Zk) o f'(Ak)</li>

<li>&delta; J =  ,,.
= &sum; _{j=1} ^{k}  &Delta;<sub>j</sub> Z<sup>T</sup><sub>j-1</sub> &sdot; dw<sub>j</sub>  + &delta;<sub>j</sub> 1 &sdot; db<sub>j</sub></li>
</ul>



<ul class="org-ul">
<li>hidden layer : feature. -&gt; regression</li>

<li>forward, back again and gain</li>

<li>verify your &delta; J / &delta; w is true?    make a small &Delta;</li>

<li>Single hidden layer NN + sigmoid function =&gt; Approximate any continuous functions =&gt; ||fx - gx|| &lt; e   (has strong expression now, but why people use multiple layer?)

<ul class="org-ul">
<li>learning single hidden layer NN is hard,</li>
<li>hard to train</li>
<li>difficult add some prior</li>
<li>data &uarr;, layer &uarr; , performance &uarr;&#x2026;</li>
</ul></li>
</ul>


<ul class="org-ul">
<li>number of layers?  number of neurons ?  overfitting?

<ul class="org-ul">
<li>overfitting: add regularization</li>
</ul></li>
</ul>


<ul class="org-ul">
<li>layer &uarr; ,  neurons &uarr; , presentation power &uarr;</li>
</ul>
</div></li></ul>
</div>

<div id="outline-container-orgheadline13" class="outline-4">
<h4 id="orgheadline13">Auto-Encoder</h4>
<div class="outline-text-4" id="text-orgheadline13">
<ul class="org-ul">
<li>PCA, sparse representation</li>

<li>unsupervised training</li>
<li>encoder, decoder use the same function, 
<ul class="org-ul">
<li>W of encoder ==  decoder, tied weight</li>
<li>!= ,</li>
</ul></li>

<li>matrix decomposition</li>
</ul>


<ul class="org-ul">
<li>sigmoid?</li>

<li>Relu? 
<ul class="org-ul">
<li>sparse coding/ compressive sensing</li>
</ul></li>
</ul>


<ul class="org-ul">
<li>prior:  
<ul class="org-ul">
<li>sparse auto-encoders</li>
<li>denoising auto-encoder</li>
<li>stacked auto-encoder</li>
</ul></li>
</ul>


<ul class="org-ul">
<li>layer-wise learning  
<ul class="org-ul">
<li>AE, RBM still adopt it</li>
</ul></li>

<li>Initialization !!  -&gt; BP(GD)
<ul class="org-ul">
<li>Good Initialization -&gt; Good Performance</li>
</ul></li>
</ul>


<ul class="org-ul">
<li>when shall we use AE?
<ul class="org-ul">
<li>classification</li>
</ul></li>

<li>fine-tuning
<ul class="org-ul">
<li>using the label info to update the parameters in AE</li>
<li>layer-wise learning -&gt;  Initialization of &theta;</li>
</ul></li>
</ul>


<ul class="org-ul">
<li>regularizer  (sparse,  ||w||<sup>2</sup><sub>F</sub> (weight decay),  )</li>
</ul>
</div>



<ul class="org-ul"><li><a id="orgheadline14"></a>denoising AE<br  /></li></ul>
</div>
</div>
</div>



<div id="outline-container-orgheadline17" class="outline-2">
<h2 id="orgheadline17">CNN <span class="timestamp-wrapper"><span class="timestamp">&lt;2015-10-15 Thu&gt;</span></span></h2>
<div class="outline-text-2" id="text-orgheadline17">
<ul class="org-ul">
<li>image classification</li>

<li>key points detector</li>

<li>2012 CNN
end-to-end learning
DNN</li>

<li>DNN : neuron</li>
</ul>
</div>
</div>



<div id="outline-container-orgheadline22" class="outline-2">
<h2 id="orgheadline22">Tricks<span class="timestamp-wrapper"><span class="timestamp">&lt;2015-10-29 Thu&gt;</span></span></h2>
<div class="outline-text-2" id="text-orgheadline22">
</div><div id="outline-container-orgheadline18" class="outline-3">
<h3 id="orgheadline18">dot not convergence</h3>
<div class="outline-text-3" id="text-orgheadline18">
<p>
Might be:
</p>

<ul class="org-ul">
<li>Initialization misuse</li>
<li>objective misuse</li>
<li>learning rate too large</li>
<li>network structure (too many units)</li>
</ul>
</div>
</div>

<div id="outline-container-orgheadline19" class="outline-3">
<h3 id="orgheadline19">learning rate</h3>
<div class="outline-text-3" id="text-orgheadline19">
<p>
learning rate :  after every n epochs, delete the rate with k.
</p>

<p>
train / validation / test
</p>

<p>
validation: if the accuracy on validation set doesn't change any more, learning rate = learning rate/10.
</p>

<p>
Do not train test data!!
</p>
</div>
</div>


<div id="outline-container-orgheadline20" class="outline-3">
<h3 id="orgheadline20">GD method</h3>
<div class="outline-text-3" id="text-orgheadline20">
<ul class="org-ul">
<li>GD ／ SGD （gradient descent)</li>
<li>batch-based GD:
<ul class="org-ul">
<li>include all classes</li>
<li>small batch</li>
<li>epoch</li>
</ul></li>
<li>momentum update</li>
<li>nesterov momentum</li>
</ul>

<p>
normalized?
</p>

<p>
LBFGS: auto-encoder is faster GD, but in CNN it is slow.
</p>

<p>
Ada grad: adaptive gradient descent.
</p>
<ul class="org-ul">
<li>cache(t+1) = cache(t) + dx<sup>2</sup></li>
<li>x(t+1) = x(t) - \(\alpha\) dx / \(\sqrt{cache(t+1) + e}\)</li>
</ul>

<p>
RMSprop
</p>
</div>
</div>

<div id="outline-container-orgheadline21" class="outline-3">
<h3 id="orgheadline21">Initialization</h3>
<div class="outline-text-3" id="text-orgheadline21">
<ul class="org-ul">
<li>N(0,1) * \(\sqrt{\frac{2}{#input}}\) 
<ul class="org-ul">
<li>avoid gradient vanishing</li>
</ul></li>

<li>model ensembles</li>

<li>data augmentation: flip.  enlarge the image and crop the image</li>
</ul>
</div>
</div>
</div>
</div>
<div id="postamble" class="status">
<p id="update">Last Updated : 2015-11-26 Thu 00:53 , Created by <a href="http://www.gnu.org/software/emacs/">Emacs</a> 24.5.1 (<a href="http://orgmode.org">Org</a> mode 8.3.2)</p>
</div>
</body>
</html>

<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
"http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en">
<head>
<!-- 2015-11-26 Thu 01:02 -->
<meta  http-equiv="Content-Type" content="text/html;charset=utf-8" />
<meta  name="viewport" content="width=device-width, initial-scale=1" />
<title>Note of Reading Group - De-anonymizing</title>
<meta  name="generator" content="Org-mode" />
<meta  name="author" content="Spinpx" />
<link rel="stylesheet" type="text/css" href="http://spinpx.com/org.css"/>
<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        displayAlign: "center",
        displayIndent: "0em",

        "HTML-CSS": { scale: 100,
                        linebreaks: { automatic: "false" },
                        webFont: "TeX"
                       },
        SVG: {scale: 100,
              linebreaks: { automatic: "false" },
              font: "TeX"},
        NativeMML: {scale: 100},
        TeX: { equationNumbers: {autoNumber: "AMS"},
               MultLineWidth: "85%",
               TagSide: "right",
               TagIndent: ".8em"
             }
});
</script>
<script type="text/javascript"
        src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS_HTML"></script>
</head>
<body>
<div id="preamble" class="status">
<div class = "nav">
    <ul>
    <li><a class = "logo" href="/index.html">SPinPx</a><li>
	<li><a href="/posts.html"> Posts </a></li>
	<li><a href="/notes.html"> Notes </a></li>
	<li><a href="/gists.html"> Gists </a></li>
	<li><a href="/about.html"> About </a></li>
    </ul>
</div>
</div>
<div id="content">
<h1 class="title">Note of Reading Group - De-anonymizing</h1>

<div id="outline-container-orgheadline1" class="outline-2">
<h2 id="orgheadline1">Paper</h2>
<div class="outline-text-2" id="text-orgheadline1">
<ul class="org-ul">
<li><a href="https://www.usenix.org/system/files/conference/usenixsecurity15/sec15-paper-caliskan-islam.pdf">sec15-paper-caliskan-islam.pdf</a></li>
</ul>
</div>
</div>

<div id="outline-container-orgheadline3" class="outline-2">
<h2 id="orgheadline3">AST (abstract syntax tree)</h2>
<div class="outline-text-2" id="text-orgheadline3">
</div><div id="outline-container-orgheadline2" class="outline-3">
<h3 id="orgheadline2">Joern</h3>
<div class="outline-text-3" id="text-orgheadline2">
<ul class="org-ul">
<li>use <a href="http://joern.readthedocs.org/en/latest/">Joern</a> as fuzz parser.</li>
<li>Def of Joern : Joern is a platform for robust analysis of C/C++ code. It generates <b>code property graphs</b>, a novel graph representation of code that exposes the code’s syntax, control-flow, data-flow and type information.</li>
<li>From this paper: Modeling and Discovering Vulnerabilities with Code Property Graphs</li>
<li>Code property graph includes: AST vs. CFG(Control Flow Graphs) vs. PDG(Program Dependence Graphs)</li>
</ul>


<div class="figure">
<p><img src="./cpg.png" alt="cpg.png" />
</p>
</div>
</div>
</div>
</div>

<div id="outline-container-orgheadline6" class="outline-2">
<h2 id="orgheadline6">Decision trees</h2>
<div class="outline-text-2" id="text-orgheadline6">
</div><div id="outline-container-orgheadline4" class="outline-3">
<h3 id="orgheadline4"><a href="https://en.wikipedia.org/wiki/Entropy_(information_theory)">Entropy (information theory)</a></h3>
<div class="outline-text-3" id="text-orgheadline4">
<ul class="org-ul">
<li><b>Entropy</b> (more specifically, Shannon entropy) is the expected value (average) of the information contained in each message.</li>
<li>\(H(X) = E[I(x)] = E[-ln(P(X))]\)</li>
<li>\(H(X) = \sum P(x_{i})I(x_{i}) = -\sum P(x_{i})log_{b}P(x_{i})\).</li>
</ul>
</div>
</div>
<div id="outline-container-orgheadline5" class="outline-3">
<h3 id="orgheadline5">How do we construct the tree? How to pick nodes in tree?</h3>
<div class="outline-text-3" id="text-orgheadline5">
<ul class="org-ul">
<li>Q: Do an attribute should be a node?</li>
<li>Q: The order of pick nodes?</li>
<li>\(H(\frac{p}{p+n}, \frac{n}{p+n}) = -\frac{p}{p+n}log_{2}\frac{p}{p+n}-\frac{n}{p+n}log_{2}\frac{n}{p+n}\)</li>
<li>A chosen attribute A, with K distinct values, divides the training set E into subsets E<sub>1</sub>, &#x2026; , E<sub>K</sub>.</li>
<li>Expected Entropy (EH) : \(EH(A) = \sum_{i=1}^{K} \frac{p_{i}+n_{i}}{p+n} H(\frac{p_{i}}{p_{i}+n_{i}},\frac{n_{i}}{p_{i}+n_{i}})\)</li>
<li>Information gain(I): \(I(A) = H(\frac{p}{p+n},\frac{pn}{p+n}) - EH(A)\)</li>
<li>Ans: Choose the attribute with the largest I</li>

<li>How to create nodes if we do not have attributes?</li>
<li>How to use information gain to decide splits?</li>
</ul>
</div>
</div>
</div>

<div id="outline-container-orgheadline7" class="outline-2">
<h2 id="orgheadline7">Random forest</h2>
<div class="outline-text-2" id="text-orgheadline7">
<dl class="org-dl">
<dt>an Ensemble Method</dt><dd>Ensembles are a divide-and-conquer approach used to improve performance. The main principle behind ensemble methods is that a group of “weak learners” can come together to form a “strong learner”.</dd>
</dl>
</div>
<div id="outline-container-orgheadline8" class="outline-3">
<h3 id="orgheadline8">Build a random tree ?</h3>
<div class="outline-text-3" id="text-orgheadline8">
<ul class="org-ul">
<li>m(3) feature : pick n(2) feature at random, m(2)d -&gt; n(2)d</li>
<li>split: boolean vs. value
<ul class="org-ul">
<li>value: information game by enumeration/evaluation randomly?</li>
</ul></li>
</ul>
</div>
</div>
<div id="outline-container-orgheadline9" class="outline-3">
<h3 id="orgheadline9">Random Forest algorithm</h3>
<div class="outline-text-3" id="text-orgheadline9">
<ol class="org-ol">
<li>For b = 1 to B: //trees
<ul class="org-ul">
<li>Draw a bootstrap (bagging, replacement) sample Z* of size N from the training data.</li>
<li>Grow a random-forest tree Tb to the bootstrapped data, by recursively repeating the follwing steps for each terminal node of the tree, until the minimum node size n<sub>min</sub> is reached.
<ul class="org-ul">
<li>Select m variables at ramdom from the p variables</li>
<li>Pick the best variable/split-point among the m</li>
<li>Spilt the node into two daughter nodes</li>
</ul></li>
</ul></li>
<li>Output the snsemble of trees {T<sub>b</sub>}<sub>1</sub><sup>B</sup>.</li>

<li>Regression: \(\hat{f}_{rf}^{B}(x) = \frac{1}{B} \sum_{b=1}^{B}T_{b}(x)\)</li>
<li>Classification: Let \(\hat{C}_b{x}\) be the class prediction of the b<sub>th</sub> random-forest tree, Then \(\hat{C}_{rf}^{B}\) = majority vote \({\hat{C}_{b}(x)}_{1}^{B}\).</li>
</ol>
</div>
</div>
<div id="outline-container-orgheadline10" class="outline-3">
<h3 id="orgheadline10">Randomization</h3>
<div class="outline-text-3" id="text-orgheadline10">
<dl class="org-dl">
<dt>Ramdonized notde optimization</dt><dd>If &Gamma; is the entire set of all possible parameters &theta; then when training the j<sup>th</sup> node we only make available a small subset \(\Gamma_{j} \subset \Gamma\) of such values.</dd>
</dl>
</div>
</div>
<div id="outline-container-orgheadline15" class="outline-3">
<h3 id="orgheadline15">why random forest make sense ?</h3>
<div class="outline-text-3" id="text-orgheadline15">
</div><div id="outline-container-orgheadline11" class="outline-4">
<h4 id="orgheadline11">why works?</h4>
<div class="outline-text-4" id="text-orgheadline11">
<ul class="org-ul">
<li>Mean Squared Error = Variance + Bias<sup>2</sup></li>
<li>In particular, trees that are grown very deep tend to learn highly irregular patterns: they overfit their training sets, because they have low bias, but very high variance. Random forests are a way of averaging multiple deep decision trees, trained on different parts of the same training set, with the goal of reducing the variance. This comes at the expense of a small increase in the bias and some loss of interpretability, but generally greatly boosts the performance of the final model.</li>
<li>De-correlation gives better accuracy</li>
</ul>
</div>
</div>
<div id="outline-container-orgheadline14" class="outline-4">
<h4 id="orgheadline14">interpreting-random-forest</h4>
<div class="outline-text-4" id="text-orgheadline14">
</div><ul class="org-ul"><li><a id="orgheadline12"></a>Random forest as a black box<br  /><div class="outline-text-5" id="text-orgheadline12">
<ul class="org-ul">
<li>Since random forests are typically treated as a black box. Indeed, a forest consists of a large number of deep trees, where each tree is trained on bagged data using random selection of features, so gaining a full understanding of the decision process by examining each individual tree is infeasible.</li>
<li>Furthermore, even if we are to examine just a single tree, it is only feasible in the case where it has a small depth and low number of features. A tree of depth 10 can already have thousands of nodes, meaning that using it as an explanatory model is almost impossible.</li>
</ul>
</div></li>
<li><a id="orgheadline13"></a>Turning a black box into a white box: decision paths<br  /></li></ul>
</div>
</div>

<div id="outline-container-orgheadline16" class="outline-3">
<h3 id="orgheadline16">Estimating generalization error: Out of bag (OOB) error</h3>
<div class="outline-text-3" id="text-orgheadline16">
<ul class="org-ul">
<li>In random forests, there is no need for cross-validation or a separate test set to get an unbiased estimate of the test set error. It is estimated internally, during the run, as follows:</li>
<li>Random forests technique involves sampling of the input data with replacement (bootstrap sampling). In this sampling, about one thrird of the data is not used for training and can be used to testing.These are called the out of bag samples. Error estimated on these out of bag samples is the out of bag error.</li>
</ul>
</div>
</div>
<div id="outline-container-orgheadline17" class="outline-3">
<h3 id="orgheadline17">Advantages</h3>
<div class="outline-text-3" id="text-orgheadline17">
<ul class="org-ul">
<li>In a forest with T trees we have t &isin; {1, &#x2026;, T}. All trees are trained independently (and possibly in parallel). During testing, each test point v is simultaneously pushed through all trees (starting at the root) until it reached the corresponding leaves.</li>
</ul>
<ul class="org-ul">
<li>sample can be trained in different machine. consistency.</li>
<li>RF as smaller prediction variance and therefore usually a better general performance</li>
<li>fast, able to deal with unbalanced and missing data.</li>
</ul>
</div>
</div>
<div id="outline-container-orgheadline18" class="outline-3">
<h3 id="orgheadline18">Effect of arguments</h3>
<div class="outline-text-3" id="text-orgheadline18">
<ul class="org-ul">
<li>effect of forest size, T from small to large, more gradient</li>
<li>effect of more classes and noise</li>
<li>effect of tree depth (D), under fitting and over fitting problem.</li>
<li>effect of bagging,</li>
</ul>
</div>
</div>

<div id="outline-container-orgheadline21" class="outline-3">
<h3 id="orgheadline21">Application</h3>
<div class="outline-text-3" id="text-orgheadline21">
</div><div id="outline-container-orgheadline19" class="outline-4">
<h4 id="orgheadline19">Object detection</h4>
</div>
<div id="outline-container-orgheadline20" class="outline-4">
<h4 id="orgheadline20">Kinect</h4>
</div>
</div>
</div>

<div id="outline-container-orgheadline22" class="outline-2">
<h2 id="orgheadline22">N-gram</h2>
<div class="outline-text-2" id="text-orgheadline22">
<dl class="org-dl">
<dt>Def</dt><dd>an n-gram is a contiguous sequence of n items from a given sequence of text or speech. The items can be phonemes, syllables, letters, words or base pairs according to the application.</dd>
</dl>
</div>
<div id="outline-container-orgheadline23" class="outline-3">
<h3 id="orgheadline23">Language models</h3>
<div class="outline-text-3" id="text-orgheadline23">
<ul class="org-ul">
<li><p>
assign a probability to language
</p>
<pre class="example">
P(about five minutes) &gt; P(about five minueets)
</pre>
<ul class="org-ul">
<li>P(W) = P(w<sub>1</sub>, w<sub>2</sub>, w<sub>3</sub>, &#x2026;, w<sub>n</sub>)</li>
<li>A model that computers either: P(W) or P(wn | w<sub>1</sub>, w<sub>2</sub>, &#x2026; w<sub>n-1</sub>) is called a language model</li>
<li>P (A|B) = P(A, B) / P(B), (bays?) =&gt; P(A, B) = ..,  P(w<sub>1</sub>, w<sub>2</sub>, w<sub>3</sub>, &#x2026;) = ?</li>
</ul></li>
</ul>
</div>
<div id="outline-container-orgheadline24" class="outline-4">
<h4 id="orgheadline24">How to estimate these prob?</h4>
<div class="outline-text-4" id="text-orgheadline24">
<ul class="org-ul">
<li>P (the | its water is so transparent that) = Count ( its water is so transparent that the) / Count ( its water is so transparent that)  ????</li>
<li>No!!!! Too many possible sentences!!</li>
</ul>
</div>
</div>
<div id="outline-container-orgheadline25" class="outline-4">
<h4 id="orgheadline25">markov assumption</h4>
<div class="outline-text-4" id="text-orgheadline25">
<ul class="org-ul">
<li>P (the | its water is so transparent that) &asymp; P (the | that) &asymp; P(the | transparent that)</li>
<li>P (w<sub>1</sub> w<sub>2</sub> &#x2026; w<sub>n</sub>) &asymp; &sum; P (w<sub>i</sub> | w<sub>i-k</sub> .. w<sub>i-1</sub>)</li>
<li>P (w<sub>i</sub> | w<sub>1</sub> w<sub>2</sub> .. w<sub>i-1</sub>) &asymp; P (w<sub>i</sub> | w<sub>i-k</sub> .. w<sub>i-1</sub>)</li>
</ul>
</div>
<ul class="org-ul"><li><a id="orgheadline26"></a>simplest case<br  /><div class="outline-text-5" id="text-orgheadline26">
<ul class="org-ul">
<li>random sequence</li>
<li>P (w<sub>1</sub> w<sub>2</sub> .. w<sub>n</sub>) &asymp; &sum; P(w<sub>i</sub>)</li>
</ul>
</div></li>
<li><a id="orgheadline27"></a>Bigram model<br  /><div class="outline-text-5" id="text-orgheadline27">
<ul class="org-ul">
<li>just by the previous word.</li>
<li>P (w<sub>i</sub> | w<sub>1</sub> w<sub>2</sub> &#x2026; w<sub>i-1</sub>) &asymp; P (w<sub>i</sub> | w<sub>i-1</sub>)</li>
</ul>
</div></li>
<li><a id="orgheadline28"></a>N-gram models<br  /><div class="outline-text-5" id="text-orgheadline28">
<ul class="org-ul">
<li>can extend to trigrams, 4-grams, 5-grams</li>
<li>In general, this is an insufficient model of language because language has long-distance dependencies.</li>
<li>But we can often get away with N-gram models because local information, we get to 3 or 4 gram can solve problems.</li>
</ul>
</div></li></ul>
</div>
</div>
</div>

<div id="outline-container-orgheadline29" class="outline-2">
<h2 id="orgheadline29">Bag of words model</h2>
<div class="outline-text-2" id="text-orgheadline29">
<ul class="org-ul">
<li>Vector representation doesn't consider the ordering of words in a document</li>
</ul>
<pre class="example">
John is quicker than Mary and Mary is quicker than John have the same vectors
</pre>
</div>
<div id="outline-container-orgheadline30" class="outline-3">
<h3 id="orgheadline30">Term Frequency TF</h3>
<div class="outline-text-3" id="text-orgheadline30">
<ul class="org-ul">
<li>The term frequency TF<sub>t,d</sub> of term t in document d is defined as the number of times that t occurs in d.</li>
</ul>
</div>
<div id="outline-container-orgheadline31" class="outline-4">
<h4 id="orgheadline31">We want to use L when computing query-document match scores. But how?</h4>
<div class="outline-text-4" id="text-orgheadline31">
<ul class="org-ul">
<li>Raw term frequency is not what we want,</li>
<li>Relevance does not increase propor)onally with term frequency.</li>
<li>Log-frequency weighting: w = log (1 + tf)</li>
</ul>
</div>
</div>
</div>

<div id="outline-container-orgheadline32" class="outline-3">
<h3 id="orgheadline32">Inverse document frequency weighting</h3>
<div class="outline-text-3" id="text-orgheadline32">
<ul class="org-ul">
<li>Rare terms are more informative than frequent terms</li>
<li>A document containing this term (rare term) is very likely to be relevant to the query. We want a high weight for rare terms.</li>
<li>df (document frequency) = the number of documents that contain t</li>
<li>idf = log (N / df) (df &lt;= N, N: document collections)</li>
</ul>
</div>
</div>
</div>

<div id="outline-container-orgheadline33" class="outline-2">
<h2 id="orgheadline33">Reference</h2>
<div class="outline-text-2" id="text-orgheadline33">
<ul class="org-ul">
<li><a href="https://class.coursera.org/nlp/lecture">Coursera-NLP</a></li>
<li><a href="https://www.usenix.org/conference/usenixsecurity15/technical-sessions/presentation/caliskan-islam">De-anonymizing Programmers via Code Stylometry</a></li>
<li><a href="https://www.youtube.com/watch?v=w2OtwL5T1ow&amp;list=PLE6Wd9FR--EdyJ5lbFl8UuGjecvVw66F6">Machine learning (Random forest part) - UBC</a></li>
<li><a href="http://blog.datadive.net/interpreting-random-forests/">http://blog.datadive.net/interpreting-random-forests/</a></li>
<li><a href="https://www.stat.berkeley.edu/~breiman/OOBestimation.pdf">https://www.stat.berkeley.edu/~breiman/OOBestimation.pdf</a></li>
<li><a href="http://stat.ethz.ch/education/semesters/ss2012/ams/slides/v10.2.pdf">http://stat.ethz.ch/education/semesters/ss2012/ams/slides/v10.2.pdf</a></li>
</ul>
</div>
</div>
</div>
<div id="postamble" class="status">
<p id="update">Last Updated : 2015-11-26 Thu 01:02 , Created by <a href="http://www.gnu.org/software/emacs/">Emacs</a> 24.5.1 (<a href="http://orgmode.org">Org</a> mode 8.3.2)</p>
</div>
</body>
</html>
